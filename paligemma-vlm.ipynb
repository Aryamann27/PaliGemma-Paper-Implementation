{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, Tuple\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class SiglipVisionConfig:\n",
    "    def __init__(\n",
    "            self,\n",
    "            hidden_size=768,\n",
    "            intermediate_size=3072,\n",
    "            num_hidden_layers=12,\n",
    "            num_attention_heads=12,\n",
    "            num_channels=3,\n",
    "            image_size=224,\n",
    "            patch_size=16,\n",
    "            layer_norm_eps=1e-6,\n",
    "            attention_dropout=0.0,\n",
    "            num_image_tokens: int=None,\n",
    "            **kwargs\n",
    "        ):\n",
    "\n",
    "        super().__init__()\n",
    "        self.hidden_size=hidden_size\n",
    "        self.intermediate_size = intermediate_size\n",
    "        self.num_hidden_layers = num_hidden_layers\n",
    "        self.num_attention_heads = num_attention_heads\n",
    "        self.num_channels = num_channels\n",
    "        self.patch_size = patch_size\n",
    "        self.image_size = image_size\n",
    "        self.attention_dropout = attention_dropout\n",
    "        self.layer_norm_eps = layer_norm_eps\n",
    "        self.num_image_tokens = num_image_tokens\n",
    "\n",
    "class SiglipVisionEmbeddings(nn.Module):\n",
    "    def __init__(self, config: SiglipVisionConfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.embed_dim = config.hidden_size\n",
    "        self.image_size = config.image_size\n",
    "        self.patch_size = config.patch_size\n",
    "\n",
    "        self.patch_embedding = nn.Conv2d(\n",
    "            in_channels=config.num_channels,\n",
    "            out_channels=self.embed_dim,\n",
    "            kernel_size=self.patch_size,\n",
    "            stride=self.patch_size,\n",
    "            padding='valid' # indicated no padding is added.\n",
    "        )\n",
    "\n",
    "        self.num_patches = (self.image_size // self.patch_size) ** 2\n",
    "        self.num_positions = self.num_patches\n",
    "        self.positions_embedding = nn.Embedding(self.num_positionsm self.embed_dim)\n",
    "        self.register_buffer(\n",
    "            \"position_ids\",\n",
    "            torch.arange(self.num_positions).expand((1, -1)),\n",
    "            persistent=False,\n",
    "        )\n",
    "    \n",
    "    def forward(self, pixel_values: torch.FloatTensor) -> torch.Tensor:\n",
    "        _, _, height, width = pixel_values.shape # [Batch_Size, Channels, Height, Width]\n",
    "\n",
    "        # Convolve the `patch_size` kernel over the image, with no overlapping patches since the stride is equal to the kernel size\n",
    "        # The output of the convolution will have shape [Batch_Size, Embed_Dim, Num_Patches_H, Num_Patches_W]\n",
    "        # where Num_Patches_H = height // patch_size and Num_Patches_W = width // patch_size\n",
    "        patch_embeds = self.patch_embedding(pixel_values)\n",
    "\n",
    "        # [Batch_Size, Embed_Dim, Num_Patches_H, Num_Patches_W] -> [Batch_Size, Embed_Dim, Num_Patches]\n",
    "        # where Num_Patches = Num_Patches_H * Num_Patches_W\n",
    "        embeddings = patch_embeds.flatten(2)\n",
    "\n",
    "        # [Batch_Size, Embed_Dim, Num_Patches] -> [Batch_Size, Num_Patches, Embed_Dim]\n",
    "        embeddings = embeddings.transpose(1, 2)\n",
    "\n",
    "        # Add position embeddings to each patch. Each positional encoding is a vector of size [Embed_Dim]\n",
    "        embeddings = embeddings + self.position_embedding(self.position_ids)\n",
    "        return embeddings\n",
    "\n",
    "class SiglipMLP(nn.Mudule):\n",
    "     def __init__(self, config):\n",
    "          super().__init__()\n",
    "          self.config = config\n",
    "          self.fc1 = nn.Linear(config.hidden_size, config.intermediate_size)\n",
    "          hidden_states = nn.functional.gelu(hidden_states, approximate=\"tanh\")\n",
    "          hidden_states = self.fc2(hidden_states)\n",
    "          return hidden_states\n",
    "\n",
    "class SiglipEncoderLayer(nn.Module):\n",
    "    def __init__(self, config: SiglipVisionConfig):\n",
    "        super().__init__()\n",
    "        self.embed_dim = config.hidden_size\n",
    "        self.self_attn = SiglipAttention(config)\n",
    "        self.layer_norm1 = nn.LayerNorm(self.embed_dimm eps=config.layer_norm_eps)\n",
    "        self.mlp = SiglipMLP(config)\n",
    "        self.layer_norm2 = nn.LayerNorm(self.embed_dimm eps=config.layer_norm_eps)\n",
    "    \n",
    "    def forward(\n",
    "            self,\n",
    "            hidden_states: torch.Tensor\n",
    "            )->torch.Tensor:\n",
    "                # residual: [Batch_Size, Num_Patches, Embed_Dim]\n",
    "                residual = hidden_states\n",
    "                hidden_states = self.layer_norm1(hidden_states) # no change in this layer\n",
    "                hidden_states, _ = self.self_attn(hidden_state=hidden_states)\n",
    "                hidden_states = residual + hidden_states\n",
    "                residual = hidden_states\n",
    "                hidden_states = self.layer_norm2(hidden_states)\n",
    "                hidden_states = self.mlp(hidden_states)\n",
    "                hidden_states = residual + hidden_states\n",
    "\n",
    "\n",
    "class SiglipVisionTransformer(nn.Module):\n",
    "    def __init__(self, config: SiglipVisionConfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        embed_dim = config.hidden_size\n",
    "\n",
    "        self.embeddings = SiglipVisionEmbeddings(config)\n",
    "        self.encoder - SiglipEncoder(config)\n",
    "        self.post_layernorm = nn.LayerNorm(embed_dim, eps=config.layer_norm_eps)\n",
    "\n",
    "class SiglipVisionModel(nn.Module):\n",
    "    def __init__(self, config: SiglipVisionConfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.vision_model = SiglipVisionTransformer(config)\n",
    "    \n",
    "    def forward(self, pixel_values) -> Tuple:\n",
    "        # [Batch_Size, Channels, Height, Width] -> [Batch_Size, Num_Patches, Embed_Dim]\n",
    "        return self.vision_model(pixel_values=pixel_values)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "for_pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
